{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings on CORD-19 dataset using word2vec\n",
    "\n",
    "This notebook covers a Python-based solution to building a word representation model on the CORD-19 dataset (as of March 20, 2020) using gensim's implementation of word2vec.\n",
    "\n",
    "The notebook is divided into 2 sections:\n",
    "\n",
    "Part 1 consists of training a word2vec model (with skip-gram) from scratch on the dataset and saving to disk.\n",
    "\n",
    "Part 2 consists of loading the most recently trained word2vec model from disk and running a few semantic tasks, such as finding most similar & dissimilar words to a user entered token. A scatter plot is also generated to help visualize the word embeddings using t-SNE and PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: train a word2vec model on CORD-19 dataset, and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set directory paths on your local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update path where CORD-19 JSON documents are located\n",
    "CORD19_dir = '/../CORD-19/CORD-19 comm_use_subset/'\n",
    "\n",
    "#update path where word2vec model will be saved\n",
    "saved_model_dir = '/../word2vec models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text content from 'body_text' section of each document and pre-process using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of all CORD-19 JSON documents\n",
    "filenames = os.listdir(CORD19_dir)\n",
    "print('Number of documents retrieved: ', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open document files and append as JSON objects into a list\n",
    "all_files = []\n",
    "for filename in tqdm(filenames):\n",
    "    if filename.endswith('.json'):\n",
    "        file_ = open(CORD19_dir + filename, 'r')\n",
    "        json_file = json.load(file_)\n",
    "        all_files.append(json_file)\n",
    "        file_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probe the structure of each JSON list element\n",
    "#samplefile = all_files[0]\n",
    "#print('Python data type: ', type(samplefile))\n",
    "#print(\"Dictionary keys: \", samplefile.keys())\n",
    "\n",
    "#Probe what the body_text dictionary looks like\n",
    "#print('body_text type: ', type(samplefile['body_text']))\n",
    "#print('body_text_length: ', len(samplefile['body_text']))\n",
    "#print('body_text keys: ', samplefile['body_text'][0].keys())\n",
    "\n",
    "#print(\"body_text contents: \")\n",
    "#pprint (samplefile['body_text'][:2], depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract sections and associated text from list of paragraphs located within body_text of a document\n",
    "def getBodyText(samplefile):\n",
    "    texts = [(di['section'], di['text']) for di in samplefile['body_text']]\n",
    "    \n",
    "    texts_di = {di['section']: \"\"  for di in samplefile['body_text']}\n",
    "    for section, text in texts:\n",
    "        texts_di[section] += text\n",
    "\n",
    "    body = \"\"\n",
    "    for section, text in texts_di.items():  \n",
    "        body += section\n",
    "        body += \" \"\n",
    "        body += text\n",
    "        body += \" \"\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract full body content of all CORD-19 documents and preprocess using Gensim\n",
    "body_text_list = []\n",
    "for file_ in tqdm(all_files):\n",
    "    body_text = getBodyText(file_)\n",
    "    body_text_list.append(gensim.utils.simple_preprocess(body_text))\n",
    "    \n",
    "print ('Number of COVID-19 documents processed: ', len(body_text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train a word2vec model (using skip-gram) on the body_text content of all documents and save model to disk.\n",
    "Change hyper-parmeters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the word2vec model using skip-gram\n",
    "model = gensim.models.Word2Vec (body_text_list, size=200, window=10, min_count=2, workers=10, compute_loss=True, sg=1, hs=1)\n",
    "print ('Word2Vec training started ...')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model.train(body_text_list,total_examples=len(body_text_list),epochs=10, compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect some model diagnostics of the newly trained word2vec model\n",
    "viewModelDiagnostics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewModelDiagnostics(model):\n",
    "    print('Number of documents in corpus: ', model.corpus_count)\n",
    "    print ('Size of corpus: ', model.corpus_total_words, ' total words')\n",
    "    print ('Size of vocab: ', len(model.wv.vocab), ' tokens')\n",
    "    print('Training time: ', model.total_train_time, ' seconds')\n",
    "    print ('Training loss:', model.get_latest_training_loss())\n",
    "    print ('Number of epochs:', model.epochs)\n",
    "    print ('Size of vector:', model.vector_size)\n",
    "    print('Type of model: skip gram = ',model.sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the word2vec model to disk for future use\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False, dir=saved_model_dir) as tmp:\n",
    "    temp_filepath = tmp.name\n",
    "    model.save(temp_filepath)\n",
    "    print('model saved to: ',temp_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: load a pre-trained word2vec model from disk, perform semantic word tasks (similar & dissimilar words), and view scatter plot of these tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreive latest word2vec model filename, ignore any other filetypes in the dir\n",
    "from pathlib import Path\n",
    "\n",
    "def getLatestWord2VecFileName():\n",
    "    files = sorted(Path(saved_model_dir).iterdir(),key=os.path.getmtime, reverse=True)\n",
    "    for index, _ in enumerate (files):\n",
    "        if files[index].name.find('gensim-model-') != -1:\n",
    "             return(files[index].as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded this word2vec model:  /Users/shanerai/Desktop/Python Exercises/Covid Dataset/CORD-19/word2vec models/gensim-model-u_p3ez5g\n"
     ]
    }
   ],
   "source": [
    "#load the latest word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "latestWord2VecModel = getLatestWord2VecFileName()\n",
    "saved_model = Word2Vec.load(latestWord2VecModel)\n",
    "print('Loaded this word2vec model: ',latestWord2VecModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus:  9118\n",
      "Size of corpus:  38430241  total words\n",
      "Size of vocab:  120072  tokens\n",
      "Training time:  4663.222483659994  seconds\n",
      "Training loss: 134217728.0\n",
      "Number of epochs: 10\n",
      "Size of vector: 200\n",
      "Type of model: skip gram =  1\n"
     ]
    }
   ],
   "source": [
    "#print some diagnostics of loaded word2vec model\n",
    "viewModelDiagnostics(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform some syntactic/semantic word tasks with the trained vectors to evaluate model performance\n",
    "#function finds and prints most similar tokens/words to a given token/word\n",
    "\n",
    "def getSimilarWords(list_test_words, top_N_words, saved_model):\n",
    "    word_vectors = saved_model.wv\n",
    "    for testword in list_test_words:\n",
    "        try:\n",
    "            result = word_vectors.most_similar(positive=[testword], topn=top_N_words)\n",
    "            print('Words similar to', testword)\n",
    "            for word, score in result:\n",
    "                print('{}: {:.4f}'.format(word, score))\n",
    "        except KeyError:    \n",
    "            print('Exception: ', testword, 'is not in vocabulary!')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit/add list of tokens below as needed, and uncomment and run cell to view most similar tokens to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of test tokens to retrieve 10 words most similar to each token\n",
    "#list_test_words = ['immunosuppressive','vaccines','antibodies','incubation','covid','epidemiologic','pulmonary','mers','leukocyte', 'mutation', 'transmission', 'hepatitis','influenza','arboviruses','diabetes']\n",
    "#top_N_words = 10\n",
    "#getSimilarWords(list_test_words, top_N_words, saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a simple scatter plot to visualize most similar tokens to a target token\n",
    "#uses PCA to reduce dimension from 100 to 10, and t-SNE to plot the tokens in 2-D space\n",
    "#Output:\n",
    "#blue dot represents the target token/word as input by user\n",
    "#green dots represent most similar tokens to target token\n",
    "#red dots represent least similar tokens to target token\n",
    "\n",
    "def plotTSNEScatter(saved_model, word, list_words):\n",
    "    \n",
    "    word_labels = [word]\n",
    "  \n",
    "    colors_list = ['blue']\n",
    "    \n",
    "    arr = np.empty((0,200), dtype='float32')\n",
    "    \n",
    "    \n",
    "    arr = np.append(arr, np.array([saved_model.wv.__getitem__(word)]), axis=0)\n",
    "    #print('Shape of arr:', arr.shape, 'Size:', arr.size, 'Dimensions:', arr.ndim)    \n",
    "\n",
    "\n",
    "    close_words = saved_model.wv.most_similar(word)\n",
    "\n",
    "    for word_score in close_words:\n",
    "        word_labels.append(word_score[0])\n",
    "        word_vector = saved_model.wv.__getitem__(word_score[0])\n",
    "        arr = np.append(arr, np.array([word_vector]), axis=0)\n",
    "        colors_list.append('green')\n",
    "\n",
    "\n",
    "    for word_ in list_words:\n",
    "        word_labels.append(word_)\n",
    "        word_vector = saved_model.wv.__getitem__(word_)\n",
    "        arr = np.append(arr, np.array([word_vector]), axis=0)\n",
    "        colors_list.append('red')\n",
    "\n",
    "\n",
    "    #using PCA to reduce dimensions from 200 to 10 (number of similar words is 10)\n",
    "    reduction = PCA(n_components=10).fit_transform(arr)\n",
    "\n",
    "    #Find t-SNE coordinates for 2 dimension space\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=10, n_iter=3000, learning_rate=300,verbose=1).fit_transform(reduction)\n",
    "    #print('Size of Y: ', Y[0][0])\n",
    "\n",
    "    x_coords = Y[:,0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    #display scatter plot\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(x_coords, y_coords, c=colors_list)\n",
    "\n",
    "    #get labels for each point\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x,y), xytext=(5,0), textcoords='offset points')\n",
    "\n",
    "    #Set the x and y limits of each of the 2 axes based on \n",
    "    plt.xlim(x_coords.min()-10, x_coords.max()+7.)\n",
    "    plt.ylim(y_coords.min()-10, y_coords.max()+7.)\n",
    "    plt.title('t-SNE scatter plot of 10 most similar & dissimilar words to {}'.format(word))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive widget: enter a word in the texbox to view plot of 10 most similar & dissimilar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd960ecb79f1409d8509d6b418ff3758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Type a word below and hit enter/return to view plot of 10 most similar & dissimilaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397ccba4ee9a49d7b0084375722c5bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='e.g. epidemic')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d018c72adb84521b066d6ef706a3a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "label = widgets.HBox([widgets.Label(value='Type a word below and hit enter/return to view plot of 10 most similar & dissimilar words:')])\n",
    "display(label)\n",
    "\n",
    "text = widgets.Text(placeholder='e.g. epidemic', disabled=False)\n",
    "display(text)\n",
    "\n",
    "output = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(output)\n",
    "\n",
    "def handle_submit(widget):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if text.value == '':\n",
    "            print('No word was entered. Enter a word in above textbox.')\n",
    "        else:\n",
    "            try:\n",
    "                plotTSNEScatter(saved_model, text.value,[word[0] for word in saved_model.wv.most_similar(negative=[text.value])])\n",
    "            except KeyError:\n",
    "                print(text.value, 'does not exist in the vocabulary. Try another word.')\n",
    "    \n",
    "text.on_submit(handle_submit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
